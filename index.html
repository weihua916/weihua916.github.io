<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Weihua Hu (胡 緯華)</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Weihua Hu (胡 緯華)</h1>
</div>
<table class="imgtable"><tr><td>
<img src="self.jpg" alt="400px" width="400px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"></td></tr></table>
<p>I am currently working at <a href="https://www.perplexity.ai/">Perplexity AI</a> to build an AI answer engine.</p>
<p>Previously, I worked at <a href="https://kumo.ai/">Kumo AI</a> to productionize Graph Neural Networks (GNNs) on modern relational databases.
I received a Ph.D. degree from the Department of Computer Science at Stanford, advised by Prof. <a href="https://cs.stanford.edu/people/jure/">Jure Leskovec</a>. I built machine learning theory/methods/benchmarks for graph-structured data, aiming to improve diverse real-world applications, such as recommender systems, drug/material discovery, and weather forecasting. I am excited about applying (graph) machine learning to solve interesting and important real-world problems. <br /> </p>
<p>I received a B.E. in Mathematical Engineering in 2016, and an M.S. in Computer Science in 2018, both from the University of Tokyo, where I worked with Prof. <a href="http://www.ms.k.u-tokyo.ac.jp/sugi/index.html">Masashi Sugiyama</a> on machine learning and Prof. <a href="http://hirosuke.mydns.jp/">Hirosuke Yamamoto</a> on information theory.
I also worked with Prof. <a href="http://www.nactem.ac.uk/tsujii/home-tsujii/atlab.html">Jun'ichi Tsujii</a> and Prof. <a href="http://researchmap.jp/hideki_mima/">Hideki Mima</a> on natural language processing.</p>
<p>[<a href="hu_cv.pdf">CV (out-dated)</a>] [<a href="https://scholar.google.co.jp/citations?user=wAFMjfkAAAAJ&amp;hl=en">Google Scholar</a>]</p>
<h2>Publications</h2>
<h3>Preprint</h3>
<ol>
<li><p><b>Weihua Hu</b>, Kaidi Cao, Kexin Huang, Edward W Huang, Karthik Subbian, Kenji Kawaguchi, Jure Leskovec. <br />
TuneUp: A Simple Improved Training Strategy for Graph Neural Networks. <br />
[<a href="https://arxiv.org/abs/2210.14843">arXiv</a>]</p>
</li>
</ol>
<h3>2025</h3>
<ol>
<li><p>Yiwen Yuan, Zecheng Zhang, Xinwei He, Akihiro Nitta, <b>Weihua Hu</b>, Dong Wang, Manan Shah, Shenyang Huang, Blaž Stojanovič, Alan Krumholz, Jan Eric Lenssen, Jure Leskovec, Matthias Fey. <br />
ContextGNN: Beyond Two-Tower Recommendation Systems. <br />
International Conference on Learning Representations (ICLR), 2025. <b>(oral)</b><br />
[<a href="https://arxiv.org/abs/2411.19513">arXiv</a>]</p>
</li>
</ol>
<h3>2024</h3>
<ol>
<li><p>Joshua Robinson*, Rishabh Ranjan*, <b>Weihua Hu</b>*, Kexin Huang*, Jiaqi Han, Alejandro Dobles, Matthias Fey, Jan Eric Lenssen, Yiwen Yuan, Zecheng Zhang, Xinwei He, Jure Leskovec. <br />
RelBench: A Benchmark for Deep Learning on Relational Databases. <br />
Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2024. <br />
[<a href="https://arxiv.org/abs/2407.20060">arXiv</a>] [<a href="https://relbench.stanford.edu/">project page</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan, Jure Leskovec, Matthias Fey. <br />
PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning. <br />
<a href="https://table-representation-learning.github.io/">Table Representation Learning Workshop</a> at NeurIPS 2024. <b>(Best paper award)</b> <br />
[<a href="https://arxiv.org/abs/2404.00776">arXiv</a>] [<a href="https://github.com/pyg-team/pytorch-frame">code</a>]</p>
</li>
<li><p>Jialin Chen, Jan Eric Lenssen, Aosong Feng, <b>Weihua Hu</b>, Matthias Fey, Leandros Tassiulas, Jure Leskovec, Rex Ying. <br />
From Similarity to Superiority: Channel Clustering for Time Series Forecasting. <br />
Neural Information Processing Systems (NeurIPS), 2024. <br />
[<a href="https://arxiv.org/abs/2404.01340">arXiv</a>]</p>
</li>
<li><p>Matthias Fey*, <b>Weihua Hu</b>*, Kexin Huang*, Jan Eric Lenssen*, Rishabh Ranjan*, Joshua Robinson*, Rex Ying, Jiaxuan You, Jure Leskovec. <br />
Relational Deep Learning: Graph Representation Learning on Relational Databases. <br />
International Conference on Machine Learning (ICML), 2024. Position paper. <br />
[<a href="https://arxiv.org/abs/2312.04615">arXiv</a>] [<a href="https://relbench.stanford.edu/">project page</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Matthias Fey. <br />
Hybrid Graph Neural Networks for Recommendation. <br />
[<a href="https://kumo.ai/resources/blog/ns-newsarticle-why-recommendation-systems-are-better-off-using-hybrid-graph-neural-networks">Blog post</a>] [<a href="https://docs.kumo.ai/docs/hybrid-graph-neural-networks">Documentation</a>]</p>
</li>
</ol>
<h3>2023</h3>
<ol>
<li><p>Remi Lam*, Alvaro Sanchez-Gonzalez*, Matthew Willson*, Peter Wirnsberger*, Meire Fortunato*, Alexander Pritzel*, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, <b>Weihua Hu</b>, Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, Oriol Vinyals, Shakir Mohamed, Peter Battaglia. <br />
Learning skillful medium-range global weather forecasting. <br />
<b>Science</b> <br />
[<a href="https://www.science.org/doi/10.1126/science.adi2336">paper</a>] [<a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">blog</a>]
[<a href="https://github.com/google-deepmind/graphcast">code</a>]</p>
</li>
<li><p>Shenyang Huang*, Farimah Poursafaei*, Jacob Danovitch, Matthias Fey, <b>Weihua Hu</b>, Emanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, Reihaneh Rabbany. <br />
Temporal Graph Benchmark for Machine Learning on Temporal Graphs. <br />
Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2023. <br />
[<a href="https://arxiv.org/abs/2307.01026">arXiv</a>] [<a href="https://tgb.complexdatalab.com/">project page</a>] [<a href="https://github.com/shenyangHuang/TGB">code</a>] </p>
</li>
</ol>
<h3>2022</h3>
<ol>
<li><p><b>Weihua Hu</b> <br />
On the Predictive Power of Graph Neural Networks <br />
Ph.D. Thesis in Computer Science, Stanford University. <br />
<b>KDD Outstanding Doctoral Dissertation Award</b> <br />
[<a href="https://searchworks.stanford.edu/view/14423818">thesis</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Rajas Bansal, Kaidi Cao, Nikhil Rao, Karthik Subbian, Jure Leskovec. <br />
Learning Backward Compatible Embeddings. <br />
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), Applied Data Science Track, 2022. <br />
[<a href="https://arxiv.org/abs/2206.03040">arXiv</a>] [<a href="https://github.com/snap-stanford/bc-emb">code</a>]</p>
</li>
<li><p>Shiori Sagawa*, Pang Wei Koh*, Tony Lee*, Irena Gao*, Sang Michael Xie, Kendrick Shen, Ananya Kumar, <b>Weihua Hu</b>, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. <br />
Extending the WILDS benchmark for unsupervised adaptation. <br />
International Conference on Learning Representations (ICLR), 2022. <b>(oral)</b><br />
[<a href="https://arxiv.org/abs/2112.05090">arXiv</a>] [<a href="https://github.com/p-lambda/wilds">project page</a>] [<a href="https://github.com/p-lambda/wilds">code</a>]</p>
</li>
</ol>
<h3>2021</h3>
<ol>
<li><p><b>Weihua Hu</b>, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec. <br />
OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs. <br /> 
KDD Cup 2021. NeurIPS competition 2022. <br />
Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2021. <br />
[<a href="https://arxiv.org/abs/2103.09430">arXiv</a>] [<a href="https://ogb.stanford.edu/docs/lsc/">project page</a>] [<a href="https://ogb.stanford.edu/kddcup2021/">kddcup 2021</a>] [<a href="https://ogb.stanford.edu/neurips2022/">neurips 2022</a>] [<a href="https://github.com/snap-stanford/ogb">code</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec, Devi Parikh, C. Lawrence Zitnick. <br />
ForceNet: A Graph Neural Network for Large-Scale Quantum Calculations. <br /> 
<a href="https://simdl.github.io/">Deep Learning for Simulation Workshop</a> at ICLR 2021. <b>(Best paper award)</b> <br />
[<a href="https://arxiv.org/abs/2103.01436">arXiv</a>] [<a href="https://github.com/Open-Catalyst-Project/ocp">code</a>] [<a href="https://slideslive.com/38955314/forcenet-a-graph-neural-network-for-largescale-quantum-calculations?ref=speaker-22639-latest">talk</a>]</p>
</li>
<li><p>Lowik Chanussot*, Abhishek Das*, Siddharth Goyal*, Thibaut Lavril*, Muhammed Shuaibi*, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, <b>Weihua Hu</b>, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, Zachary Ulissi. <br />
The Open Catalyst 2020 (OC20) Dataset and Community Challenges. <br />
ACS Catalysis, 2021. <br />
[<a href="https://arxiv.org/abs/2010.09990">arXiv</a>] [<a href="https://opencatalystproject.org/">project page</a>] [<a href="https://github.com/Open-Catalyst-Project/ocp">code</a>]</p>
</li>
<li><p>C. Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo, Caleb Ho, <b>Weihua Hu</b>, Thibaut Lavril, Aini Palizhati, Morgane Riviere, Muhammed Shuaibi, Anuroop Sriram, Kevin Tran, Brandon Wood, Junwoong Yoon, Devi Parikh, Zachary Ulissi. <br />
An Introduction to Electrocatalyst Design using Machine Learning for Renewable Energy Storage. <br /> 
[<a href="https://arxiv.org/abs/2010.09990">arXiv</a>]</p>
</li>
<li><p>Pang Wei Koh*, Shiori Sagawa*, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, <b>Weihua Hu</b>, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, Percy Liang <br />
WILDS: A benchmark of in-the-wild distribution shifts. <br />
International Conference on Machine Learning (ICML), 2021. <b>(long talk)</b> <br />
[<a href="https://arxiv.org/abs/2012.07421">arXiv</a>] [<a href="https://wilds.stanford.edu/">project page</a>] [<a href="https://github.com/p-lambda/wilds">code</a>]</p>
</li>
</ol>
<h3>2020</h3>
<ol>
<li><p><b>Weihua Hu</b>, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec. <br />
Open Graph Benchmark: Datasets for Machine Learning on Graphs. <br />
Neural Information Processing Systems (NeurIPS), 2020. <b>(spotlight)</b> <br />
[<a href="https://arxiv.org/abs/2005.00687">arXiv</a>] [<a href="https://ogb.stanford.edu/">project page</a>] [<a href="https://github.com/snap-stanford/ogb">code</a>] [<a href="https://slideslive.com/38937788/open-graph-benchmark-datasets-for-machine-learning-on-graphs?ref=speaker-22639-latest">talk</a>]</p>
</li>
<li><p><b>Weihua Hu</b>*, Bowen Liu*, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec. <br />
Strategies for Pre-training Graph Neural Networks. <br />
International Conference on Learning Representations (ICLR), 2020. <b>(spotlight)</b> <br />
<a href="https://grlearning.github.io/">Graph Representation Learning Workshop</a> at NeurIPS 2019. <b>(oral)</b> <br />
[<a href="https://openreview.net/forum?id=HJlWWJSFDH">OpenReview</a>] [<a href="http://snap.stanford.edu/gnn-pretrain/">project page</a>] [<a href="https://github.com/snap-stanford/pretrain-gnns/">code</a>]</p>
</li>
<li><p>Hongyu Ren*, <b>Weihua Hu</b>*, Jure Leskovec. <br />
Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings. <br />
International Conference on Learning Representations (ICLR), 2020. <br />
[<a href="https://openreview.net/forum?id=BJgr4kSFDS">OpenReview</a>] [<a href="http://snap.stanford.edu/query2box/">project page</a>] [<a href="https://github.com/hyren/query2box">code</a>]</p>
</li>
</ol>
<h3>2019</h3>
<ol>
<li><p>Keyulu Xu*, <b>Weihua Hu</b>*, Jure Leskovec, Stefanie Jegelka. <br />
How Powerful are Graph Neural Networks? <br />
International Conference on Learning Representations (ICLR), 2019. <b>(oral)</b> <br />
[<a href="https://openreview.net/forum?id=ryGs6iA5Km">OpenReview</a>] [<a href="https://arxiv.org/abs/1810.00826">arXiv</a>] [<a href="https://github.com/weihua916/powerful-gnns">code</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, Masashi Sugiyama. <br />
Unsupervised Discrete Representation Learning. <br />
Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 97-119.<br />
(<b>Book chapter</b> contribution of our ICML 2017 work.)<br />
[<a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_6">Chapter</a>]</p>
</li>
</ol>
<h3>2018</h3>
<ol>
<li><p><b>Weihua Hu</b>, Gang Niu, Issei Sato, Masashi Sugiyama. <br />
Does Distributionally Robust Supervised Learning Give Robust Classifiers? <br />
International Conference on Machine Learning (ICML), 2018. <br />
[<a href="http://arxiv.org/abs/1611.02041">arXiv</a>]</p>
</li>
<li><p>Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, <b>Weihua Hu</b>, Ivor Tsang, Masashi Sugiyama. <br />
Co-teaching: Robust training of deep neural networks with noisy labels. <br />
Neural Information Processing Systems (NeurIPS), 2018. <br />
[<a href="http://arxiv.org/abs/1804.06872">arXiv</a>]</p>
</li>
</ol>
<h3>2017</h3>
<ol>
<li><p><b>Weihua Hu</b>, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, Masashi Sugiyama. <br />
Learning Discrete Representations via Information Maximizing Self Augmented Training. <br />
International Conference on Machine Learning (ICML), 2017. <br />
[<a href="http://arxiv.org/abs/1702.08720">arXiv</a>][<a href="https://github.com/weihua916/imsat">code</a>][<a href="https://vimeo.com/238230059">talk</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Hirosuke Yamamoto, Junya Honda. <br />
Worst-case Redundancy of Optimal Binary AIFV Codes and their Extended Codes.<br />
IEEE Transactions on Information Theory, vol.63, no.8, pp.5074-5086, August 2017. <br />
[<a href="http://arxiv.org/abs/1607.07247">arXiv</a>]</p>
</li>
<li><p>Takashi Ishida, Gang Niu, <b>Weihua Hu</b>, Masashi Sugiyama. <br />
Learning from Complementary Labels.<br />
Neural Information Processing Systems (NeurIPS), 2017. <br />
[<a href="http://arxiv.org/abs/1705.07541">arXiv</a>]</p>
</li>
</ol>
<h3>2016</h3>
<ol>
<li><p><b>Weihua Hu</b>, Hirosuke Yamamoto, Junya Honda. <br />
Tight Upper Bounds on the Redundancy of Optimal Binary AIFV Codes.<br />
IEEE International Symposium on Information Theory (ISIT), 2016.<br />
[<a href="papers/isit2016.pdf">paper</a>][<a href="presentation/isit_huyamhon.pdf">slide</a>]</p>
</li>
<li><p><b>Weihua Hu</b>, Jun'ichi Tsujii. <br />
A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings. <br />
The annual meeting of the Association for Computational Linguistics (ACL), 2016. <br />
[<a href="http://aclweb.org/anthology/P16-2062">paper</a>][<a href="presentation/acl_poster_final.pdf">poster</a>][<a href="https://github.com/weihua916/LCTM">code</a>]</p>
</li>
</ol>
<h2>Workshop Organization</h2>
<ul>
<li><p>Lead organizer of the <a href="https://ogb.stanford.edu/neurips2022/">Open Graph Benchmark Large-Scale Challenge</a> at the NeurIPS 2022.</p>
</li>
<li><p>Lead organizer of the <a href="https://ogb.stanford.edu/kddcup2021/">Open Graph Benchmark Large-Scale Challenge</a> at the KDD Cup 2021.</p>
</li>
</ul>
<h2>Awards</h2>
<ul>
<li><p><a href="https://forbesjapan.com/feat/30under30/2023/honorees/">Forbes 30 under 30, Japan</a> (2023)</p>
</li>
<li><p><a href="https://kdd.org/kdd2023/awards/">KDD Outstanding Doctoral Dissertation Award</a> (2023)</p>
</li>
<li><p><a href="https://masason-foundation.org/">Masason Foundation Fellowship</a> (2019&ndash;)</p>
</li>
<li><p><a href="https://www.funaifoundation.jp/scholarship/scholarship_summary.html">Funai Overseas Scholarship</a> (2018&ndash;)</p>
</li>
<li><p>Dean’s Award, Graduate School of Information Science and Technology, University of Tokyo (2018)</p>
</li>
<li><p>AIP network lab director award (2017)</p>
</li>
</ul>
<h2>Professional Services</h2>
<ul>
<li><p>Conference Reviewer (NerIPS 2019&ndash;2021, ICML 2020&ndash;2021, ICLR 2021&ndash;2022, KDD 2022&ndash;2023, AAAI 2024, ECML-PKDD 2020, ISIT 2020)</p>
</li>
<li><p>Workshop Reviewer (<a href="https://grlplus.github.io/">GRL+ workshop</a> and <a href="https://logicalreasoninggnn.github.io/">ILRGNN workshop</a> at ICML 2020)</p>
</li>
</ul>
<h2>Talk</h2>
<ul>
<li><p>December 18, 2023: Invited Talk@Tsinghua University, China</p>
</li>
<li><p>October 24, 2023: Invited Talk@Stanford Graph Learning Workshop, USA [<a href="https://www.youtube.com/watch?v=aLeDg7vl56A&amp;t=6770s">video</a>]</p>
</li>
<li><p>August 8, 2023: Keynote at Ph.D. and Undergraduate Consortiums@KDD, USA</p>
</li>
<li><p>August 7, 2023: KDD Dissertation Award Talk@KDD, USA [<a href="https://www.youtube.com/watch?v=096YNYL-iOA">video</a>]</p>
</li>
<li><p>May 16, 2023: Link Prediction with Graph Neural Networks Webinar Talk@Kumo [<a href="https://info.kumo.ai/link-prediction-with-graph-neural-networks-in-production?utm_content=246695576&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-1573529592882614272">website</a>]</p>
</li>
<li><p>December 21, 2022: Invited Talk@<a href="https://trustmlresearch.github.io/index.html">TrustML Young Scientist Seminars</a>, Japan [<a href="https://www.youtube.com/watch?v=tGyeFAccytc">video</a>]</p>
</li>
<li><p>September 28, 2022: Invited Talk@<a href="http://snap.stanford.edu/graphlearning-workshop-2022/">Stanford Graph Learning Workshop</a>, USA [<a href="https://www.youtube.com/watch?v=x-64ZFBv_G0&amp;list=PLqYw1C4YGfr0byz1XMD95YMDR-so4qy1e&amp;index=16">video</a>]</p>
</li>
<li><p>September 2, 2022: Invited Talk@Combridge University, UK</p>
</li>
<li><p>August 4, 2022: Invited Talk@<a href="https://bhanml.github.io/group.html">TMLR group</a> at HKBU, Hong Kong</p>
</li>
<li><p>June 2, 2022: Invited Talk@<a href="https://2022.baai.ac.cn/schedule">BAAI</a>, China</p>
</li>
<li><p>March 3, 2022: Invited Talk@<a href="https://grlmila.github.io/">MILA GRL Reading Group</a>, Canada [<a href="https://drive.google.com/file/d/13RRZYzmnf-1Z1fvDk3-Sn2rhepAR0bYF/view">video</a>]</p>
</li>
<li><p>September 16, 2021: Invited Talk@Stanford Graph Learning Workshop, USA [<a href="https://www.youtube.com/watch?v=wDxwkuQwOWA">video</a>]</p>
</li>
<li><p>September 2, 2021: Invited Seminar Talk@RIKEN AIP Center, Japan [<a href="https://www.youtube.com/watch?v=pSXlkw-2Z08">video</a>]</p>
</li>
<li><p>May 6, 2021: Invited Seminar Talk@Stanford CogAI lab, USA</p>
</li>
<li><p>Auguest 24, 2020: Invited Short Talk@KDD Deep Learning Day [<a href="https://www.youtube.com/watch?v=4F8qNjexZt0">video</a>]</p>
</li>
<li><p>Auguest 5, 2020: Invited Seminar Talk@RIKEN AIP Center, Japan</p>
</li>
<li><p>June 24, 2020: Invited Seminar Talk@Twitter Machine Learning Group, UK</p>
</li>
<li><p>June 23, 2020: Invited Seminar Talk@4Paradigm, Hong Kong</p>
</li>
</ul>
<h2>Teaching</h2>
<ul>
<li><p>Spring 2023: Guest Lecture at <a href="http://web.stanford.edu/class/cs224w/">CS224W</a> (Machine Learning with Graphs) at Stanford</p>
</li>
<li><p>Fall 2021: Teaching Assistant for <a href="http://web.stanford.edu/class/cs224w/">CS224W</a> (Machine Learning with Graphs) at Stanford</p>
</li>
<li><p>Winter 2021: Teaching Assistant for <a href="http://web.stanford.edu/class/cs224w/">CS224W</a> (Machine Learning with Graphs) at Stanford</p>
</li>
</ul>
<h2>Work Experiences</h2>
<ul>
<li><p>Software Engineer Intern at <a href="https://kumo.ai/">Kumo.ai</a>, Mountain View (2022.9&ndash;2022.12)</p>
</li>
<li><p>Research Intern at <a href="https://www.deepmind.com/">DeepMind</a>, London (2022.6&ndash;2022.9)</p>
</li>
<li><p>Research Intern at <a href="https://ai.facebook.com/">Facebook Artificial Intelligence Research</a>, Menlo Park (2020.6&ndash;2020.9)</p>
</li>
<li><p>Research Assistant at <a href="http://www.riken.jp/research/labs/aip/">Center for Advanced Integrated Intelligence Research</a>, RIKEN (2016.11&ndash;2018.8)</p>
</li>
<li><p>AIP Challenge (Research funding) &ldquo;Reliable machine learning in the wild&rdquo; (2016.10&ndash;2017.3)</p>
</li>
<li><p>Technical Staff at <a href="http://www.airc.aist.go.jp/">Artificial Intelligence Research Center</a>, AIST (2015.10&ndash;2016.10)</p>
</li>
<li><p>Internship at <a href="https://www.preferred-networks.jp/en/">Preferred Networks</a> (2016.8&ndash;2016.9)</p>
</li>
<li><p>Part-time Engineer at <a href="https://preferred.jp/">Preferred Infrastructure</a> (2015.11&ndash;2016.3)</p>
</li>
</ul>
<h2>Contact</h2>
<p>Email: weihuahu [at] cs.stanford.edu <br />
URL: <a href="https://weihua916.github.io/">https://weihua916.github.io/</a> <br />
Github: <a href="https://github.com/weihua916/">https://github.com/weihua916/</a></p>
<div id="footer">
<div id="footer-text">
Page generated 2025-01-22 21:11:30 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
